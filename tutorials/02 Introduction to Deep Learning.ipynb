{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction to Deep Learning\n",
    "\n",
    "## What is Deep Learning?\n",
    "\n",
    "Deep learning can be understood as a set of algorithms that were developed to train artificial neural networks with many layers most efficiently.\n",
    "\n",
    "Artificial neurons represent the building blocks of the multilayer artificial neural networks. The basic concept behind artificial neural networks was built upon hypotheses and models of how the human brain works to solve complex problem tasks. Although artificial neural networks have gained a lot of popularity in recent years, early studies of neural networks go back to the 1940s when Warren McCulloch and Walter Pitt first described how neurons could work.\n",
    "\n",
    "However, in the decades that followed the first implementation of the McCulloch- Pitt neuron model—Rosenblatt's perceptron in the 1950s, many researchers and machine learning practitioners slowly began to lose interest in neural networks since no one had a good solution for training a neural network with multiple layers. Eventually, interest in neural networks was rekindled in 1986 when D.E. Rumelhart, G.E. Hinton, and R.J. Williams were involved in the (re)discovery and popularization of the backpropagation algorithm to train neural networks more efficiently ([Learning representations by backpropagating errors, David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams, Nature, 323 (6088): 533–536, 1986](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial neuron\n",
    "\n",
    "An artifical neuron is the basic unit of a neural network. It calculates the weighted sum of its inputs and then applies an activation function to normalize the sum. The activation functions can be linear or nonlinear. Also, there are weights associated with each input of a neuron. These are the parameters which the network has to learn during the training phase.\n",
    "\n",
    "A schematic diagram of a neuron is given below.\n",
    "\n",
    "<img src=\"figures/artificial_neuron.jpg\" alt=\"Single-layer network\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "\n",
    "The activation function is used as a decision making body at the output of a neuron. The neuron learns Linear or Non-\n",
    "linear decision boundaries based on the activation function. It also has a normalizing effect on the neuron output which prevents the output of neurons after several layers to become very large, due to the cascading effect. There are three most widely used activation functions.\n",
    "\n",
    "1. Sigmoid\n",
    "It maps the input ( x axis ) to values between 0 and 1.\n",
    "\n",
    "<img src=\"figures/activation_sigmoid.png\" alt=\"Activation sigmoid\" style=\"width: 400px;\"/>\n",
    "\n",
    "2. Tanh\n",
    "It is similar to the sigmoid function butmaps the input to values between -1 and 1.\n",
    "\n",
    "<img src=\"figures/activation_tanh.png\" alt=\"Activation tanh\" style=\"width: 400px;\"/>\n",
    "\n",
    "3. Rectified Linear Unit (ReLU)\n",
    "It allows only positive values to pass through it. The negative values are mapped to zero.\n",
    "\n",
    "<img src=\"figures/activation_relu.png\" alt=\"Activation ReLU\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-layer neural perceptron\n",
    "\n",
    "The simplest kind of neural network is a single-layer perceptron network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. In this way it can be considered the simplest kind of feed-forward network.\n",
    "\n",
    "The sum of the products of the weights and the inputs is calculated in each node, and if the value is above some threshold (typically 0) the neuron fires and takes the activated value (typically 1); otherwise it takes the deactivated value (typically -1). Neurons with this kind of activation function are also called artificial neurons or linear threshold units. In the literature the term perceptron often refers to networks consisting of just one of these units. A similar neuron was described by Warren McCulloch and Walter Pitts in the 1940s.\n",
    "\n",
    "<img src=\"figures/single_layer_network_1.jpg\" alt=\"Single-layer network\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer neural perceptron\n",
    "\n",
    "A multilayer perceptron (MLP) is a class of feedforward artificial neural network. An MLP consists of at least three layers of nodes. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.\n",
    "\n",
    "<img src=\"figures/multi_layer_network.jpg\" alt=\"Multi-layer network\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward neural network\n",
    "\n",
    "Feedforward neural networks are the most common networks used in Deep Learning.\n",
    "\n",
    "In this type of architecture, a connection between two nodes is only permitted from nodes in layer i to nodes in layer i + 1 (hence the term feedforward; there are no backwards or inter-layer connections allowed).\n",
    "\n",
    "Furthermore, the nodes in layer i are fully connected to the nodes in layer i + 1. This implies that every node in layer i connects to every node in layer i + 1. For example, in the figure above, there are a total of 2 x 3 = 6 connections between layer 0 and layer 1 — this is where the term “fully connected” or “FC” for short, comes from.\n",
    "\n",
    "<img src=\"figures/simple_neural_network_feedforward.png\" alt=\"Feedforward network\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a feedforward network with Keras\n",
    "\n",
    "We are going to use the [Kaggle Dogs vs. Cats classification challenge](https://www.kaggle.com/c/dogs-vs-cats/data) and create a Feedforward network to classify images between dogs and cats.\n",
    "\n",
    "The goal of this challenge is to correctly classify whether a given image contains a dog or a cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def image_to_feature_vector(image, size=(32, 32)):\n",
    "    # resize the image to a fixed size, then flatten the image into\n",
    "    # a list of raw pixel intensities\n",
    "    return cv2.resize(image, size).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 1000/25000\n",
      "[INFO] processed 2000/25000\n",
      "[INFO] processed 3000/25000\n",
      "[INFO] processed 4000/25000\n",
      "[INFO] processed 5000/25000\n",
      "[INFO] processed 6000/25000\n",
      "[INFO] processed 7000/25000\n",
      "[INFO] processed 8000/25000\n",
      "[INFO] processed 9000/25000\n",
      "[INFO] processed 10000/25000\n",
      "[INFO] processed 11000/25000\n",
      "[INFO] processed 12000/25000\n",
      "[INFO] processed 13000/25000\n",
      "[INFO] processed 14000/25000\n",
      "[INFO] processed 15000/25000\n",
      "[INFO] processed 16000/25000\n",
      "[INFO] processed 17000/25000\n",
      "[INFO] processed 18000/25000\n",
      "[INFO] processed 19000/25000\n",
      "[INFO] processed 20000/25000\n",
      "[INFO] processed 21000/25000\n",
      "[INFO] processed 22000/25000\n",
      "[INFO] processed 23000/25000\n",
      "[INFO] processed 24000/25000\n"
     ]
    }
   ],
   "source": [
    "# initialize the data matrix and labels list\n",
    "data = []\n",
    "labels = []\n",
    "train_path='/Volumes/Data/Computer_Vision/kaggle_dogs_cats/train'\n",
    "\n",
    "imagePaths = list(paths.list_images(train_path))\n",
    "# loop over the input images\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # load the image and extract the class label (assuming that our\n",
    "    # path as the format: /path/to/dataset/{class}.{image_num}.jpg\n",
    "    image = cv2.imread(imagePath)\n",
    "    label = imagePath.split(os.path.sep)[-1].split(\".\")[0]\n",
    "    \n",
    "    # construct a feature vector raw pixel intensities, then update\n",
    "    # the data matrix and labels list\n",
    "    features = image_to_feature_vector(image)\n",
    "    data.append(features)\n",
    "    labels.append(label)\n",
    "    \n",
    "    # show an update every 1,000 images\n",
    "    if i > 0 and i % 1000 == 0:\n",
    "        print(\"[INFO] processed {}/{}\".format(i, len(imagePaths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] constructing training/testing split...\n"
     ]
    }
   ],
   "source": [
    "# encode the labels, converting them from strings to integers\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    " \n",
    "# scale the input image pixels to the range [0, 1], then transform\n",
    "# the labels into vectors in the range [0, num_classes] -- this\n",
    "# generates a vector for each label where the index of the label\n",
    "# is set to `1` and all other entries to `0`\n",
    "data = np.array(data) / 255.0\n",
    "labels = np_utils.to_categorical(labels, 2)\n",
    " \n",
    "# partition the data into training and testing splits, using 75%\n",
    "# of the data for training and the remaining 25% for testing\n",
    "print(\"[INFO] constructing training/testing split...\")\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(\n",
    "    data, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the architecture of the network\n",
    "model = Sequential()\n",
    "model.add(Dense(768, input_dim=3072, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
    "model.add(Dense(384, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmunoz/.pyenv/versions/3.6.1/envs/keras/lib/python3.6/site-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.6853 - acc: 0.5629    \n",
      "Epoch 2/50\n",
      "18750/18750 [==============================] - 9s - loss: 0.6652 - acc: 0.5975     \n",
      "Epoch 3/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.6539 - acc: 0.6129    \n",
      "Epoch 4/50\n",
      "18750/18750 [==============================] - 9s - loss: 0.6443 - acc: 0.6275     \n",
      "Epoch 5/50\n",
      "18750/18750 [==============================] - 15s - loss: 0.6399 - acc: 0.6361    \n",
      "Epoch 6/50\n",
      "18750/18750 [==============================] - 18s - loss: 0.6369 - acc: 0.6335    \n",
      "Epoch 7/50\n",
      "18750/18750 [==============================] - 14s - loss: 0.6293 - acc: 0.6451    \n",
      "Epoch 8/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.6270 - acc: 0.6478    \n",
      "Epoch 9/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.6222 - acc: 0.6538    \n",
      "Epoch 10/50\n",
      "18750/18750 [==============================] - 13s - loss: 0.6184 - acc: 0.6559    \n",
      "Epoch 11/50\n",
      "18750/18750 [==============================] - 21s - loss: 0.6143 - acc: 0.6625    \n",
      "Epoch 12/50\n",
      "18750/18750 [==============================] - 17s - loss: 0.6096 - acc: 0.6667    \n",
      "Epoch 13/50\n",
      "18750/18750 [==============================] - 15s - loss: 0.6087 - acc: 0.6655    \n",
      "Epoch 14/50\n",
      "18750/18750 [==============================] - 15s - loss: 0.6064 - acc: 0.6691    \n",
      "Epoch 15/50\n",
      "18750/18750 [==============================] - 16s - loss: 0.6008 - acc: 0.6756    \n",
      "Epoch 16/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5974 - acc: 0.6764    \n",
      "Epoch 17/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5932 - acc: 0.6828    \n",
      "Epoch 18/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5931 - acc: 0.6836    \n",
      "Epoch 19/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5904 - acc: 0.6851    \n",
      "Epoch 20/50\n",
      "18750/18750 [==============================] - 13s - loss: 0.5844 - acc: 0.6873    \n",
      "Epoch 21/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5820 - acc: 0.6921    \n",
      "Epoch 22/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5803 - acc: 0.6929    \n",
      "Epoch 23/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5754 - acc: 0.7022    \n",
      "Epoch 24/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5716 - acc: 0.6991    \n",
      "Epoch 25/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5699 - acc: 0.7061    \n",
      "Epoch 26/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5667 - acc: 0.7051    \n",
      "Epoch 27/50\n",
      "18750/18750 [==============================] - 12s - loss: 0.5624 - acc: 0.7117    \n",
      "Epoch 28/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5603 - acc: 0.7130    \n",
      "Epoch 29/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5554 - acc: 0.7167    \n",
      "Epoch 30/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5519 - acc: 0.7209    \n",
      "Epoch 31/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5521 - acc: 0.7211    \n",
      "Epoch 32/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5499 - acc: 0.7216    \n",
      "Epoch 33/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5419 - acc: 0.7327    \n",
      "Epoch 34/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5391 - acc: 0.7299    \n",
      "Epoch 35/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5368 - acc: 0.7339    \n",
      "Epoch 36/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5367 - acc: 0.7277    - ETA: 0s - loss: 0.5364 \n",
      "Epoch 37/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5290 - acc: 0.7373    \n",
      "Epoch 38/50\n",
      "18750/18750 [==============================] - 9s - loss: 0.5239 - acc: 0.7424     \n",
      "Epoch 39/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5248 - acc: 0.7394    \n",
      "Epoch 40/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.5205 - acc: 0.7416    \n",
      "Epoch 41/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5206 - acc: 0.7459    \n",
      "Epoch 42/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5104 - acc: 0.7561    \n",
      "Epoch 43/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5144 - acc: 0.7502    \n",
      "Epoch 44/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5077 - acc: 0.7524    \n",
      "Epoch 45/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5033 - acc: 0.7604    \n",
      "Epoch 46/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.5050 - acc: 0.7580    \n",
      "Epoch 47/50\n",
      "18750/18750 [==============================] - 9s - loss: 0.4947 - acc: 0.7645     \n",
      "Epoch 48/50\n",
      "18750/18750 [==============================] - 9s - loss: 0.4958 - acc: 0.7603     \n",
      "Epoch 49/50\n",
      "18750/18750 [==============================] - 11s - loss: 0.4852 - acc: 0.7683    \n",
      "Epoch 50/50\n",
      "18750/18750 [==============================] - 10s - loss: 0.4899 - acc: 0.7675    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11cca1ac8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using SGD\n",
    "print(\"[INFO] compiling model...\")\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "model.fit(trainData, trainLabels, epochs=50, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating on testing set...\n",
      "6250/6250 [==============================] - 1s     \n",
      "[INFO] loss=0.6906, accuracy: 62.4480%\n"
     ]
    }
   ],
   "source": [
    "# show the accuracy on the testing set\n",
    "print(\"[INFO] evaluating on testing set...\")\n",
    "(loss, accuracy) = model.evaluate(testData, testLabels, batch_size=128, verbose=1)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "On a Titan X GPU, the entire process of feature extraction, training the neural network, and evaluation took a total of 1m 15s with each epoch taking less than 0 seconds to complete.\n",
    "\n",
    "At the end of the 50th epoch, we see that we are getting ~76% accuracy on the training data and 67% accuracy on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
